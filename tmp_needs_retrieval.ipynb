{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from csv import writer\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random, csv\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "NEED_TERMS = set(['need', 'needs', 'needed', 'needing'])\n",
    "\n",
    "def myfunc(first, second, third) :\n",
    "    if __name__==\"__main__\":\n",
    "\n",
    "        INPUT_FILE = first\n",
    "        OUTPUT_FILE = os.path.join(\"gsndData\", second)\n",
    "        NUM_SENTS =  int(third)\n",
    "\n",
    "        tokenizer = TweetTokenizer()\n",
    "\n",
    "        need_sents = []\n",
    "        with open(INPUT_FILE, 'r') as f:\n",
    "            with open(OUTPUT_FILE, 'w') as o:\n",
    "                for line in f:\n",
    "                    sentences = sent_tokenize(line)\n",
    "                    for raw_sent in sentences:\n",
    "                        raw_tokens = tokenizer.tokenize(raw_sent)\n",
    "\n",
    "                        if len(NEED_TERMS.intersection(raw_tokens)) > 0:\n",
    "                            need_sents.append(raw_sent)\n",
    "        \n",
    "        sample = random.choices(need_sents, k = NUM_SENTS)\n",
    "        with open(OUTPUT_FILE, 'w') as o:\n",
    "            writer = csv.writer(o)\n",
    "            writer.writerow(['id', 'text'])\n",
    "            for id_, sentence in enumerate(sample):\n",
    "                writer.writerow([id_, sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time, ahocorasick\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# read phrases file\n",
    "def priority(first, second, third):\n",
    "\n",
    "    NEED_TERMS = ['needs', 'supplies']\n",
    "\n",
    "    \n",
    "    def read_phrases(PHRASES_FILE):\n",
    "        # ahocorasick: faster way of searching for phrases in text\n",
    "        phrase_search = {}\n",
    "        for i in range(MIN_PHRASE_LENGTH, MAX_PHRASE_LENGTH + 1):\n",
    "            phrase_search[i] = ahocorasick.Automaton()\n",
    "\n",
    "        all_phrases = []\n",
    "        with open(PHRASES_FILE, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split('\\t')\n",
    "                if len(line) > 1 and float(line[0]) >= PHRASE_THRESHOLD:\n",
    "                    if line[1].endswith(\"'s\"):\n",
    "                        line[1] = line[1][:-2]\n",
    "                    \n",
    "                    tokens = tokenizer.tokenize(line[1])\n",
    "                    if len(tokens) <= MAX_PHRASE_LENGTH and len(tokens) >= MIN_PHRASE_LENGTH:\n",
    "                        phrase = '-'.join(tokens)\n",
    "\n",
    "                        phrase_search[len(tokens)].add_word(line[1], (line[1], phrase))\n",
    "\n",
    "                        all_phrases.append(phrase)\n",
    "\n",
    "        for i in range(MIN_PHRASE_LENGTH, MAX_PHRASE_LENGTH + 1):\n",
    "            phrase_search[i].make_automaton()\n",
    "                        \n",
    "        return all_phrases, phrase_search\n",
    "\n",
    "    # annotate phrases in text so they are kept as unigrams\n",
    "    def annotate_phrases(raw_sent, phrases):\n",
    "        phrase_sent = raw_sent.lower()\n",
    "        for i in range(MAX_PHRASE_LENGTH , MIN_PHRASE_LENGTH - 1, -1):\n",
    "            if len(phrases[i]) > 0:\n",
    "                phrases_in_sent = {}\n",
    "                for end_index, (phrase, combined_phrase) in phrases[i].iter(phrase_sent):\n",
    "                    phrases_in_sent[phrase] = combined_phrase\n",
    "\n",
    "                for phrase in sorted(phrases_in_sent, key = len, reverse = True):\n",
    "                    phrase_sent = phrase_sent.replace(phrase, phrases_in_sent[phrase])\n",
    "\n",
    "        return phrase_sent\n",
    "\n",
    "    def get_nouns(vocab_pos, phrases):\n",
    "        nouns = set()\n",
    "        for word, tags in vocab_pos.items():\n",
    "            # if word == 'needed':\n",
    "            #     print(tags)\n",
    "            # if word is more frequently used as a noun, add it to list of nouns\n",
    "            if max(tags, key = tags.get) in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "                nouns.add(word)\n",
    "\n",
    "        for phrase in phrases:\n",
    "            tokens = phrase.split('-')\n",
    "            final_token = tokens[-1]\n",
    "\n",
    "            if final_token in nouns:\n",
    "                nouns.add(phrase)\n",
    "\n",
    "        return nouns\n",
    "\n",
    "    # find all nouns closest to NEED_TERMS\n",
    "    def get_ranked_needs(emb_model, nouns, output_file, top = 100):\n",
    "        top_noun_count = 0\n",
    "        with open(os.path.join(output_file), 'w') as o:\n",
    "            for (term, score) in emb_model.wv.most_similar(positive = NEED_TERMS, topn = top * 2):\n",
    "                if term in nouns:\n",
    "                    top_noun_count += 1\n",
    "                    o.write(term + '\\n')\n",
    "\n",
    "                    if top_noun_count == top:\n",
    "                        break\n",
    "\n",
    "    if __name__==\"__main__\":\n",
    "        start_time = time.time()\n",
    "\n",
    "        INPUT_FILE = first\n",
    "        OUTPUT_FOLDER = second\n",
    "        OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, 'sentences.json')\n",
    "        NEEDS_FILE = os.path.join(OUTPUT_FOLDER, 'priority-needs.txt')\n",
    "\n",
    "        PHRASES_FILE = third\n",
    "        PHRASE_THRESHOLD = 0.8\n",
    "        MIN_PHRASE_LENGTH = 2\n",
    "        MAX_PHRASE_LENGTH = 5\n",
    "\n",
    "        # if len(sys.argv) < 5:\n",
    "        TOPN = 100\n",
    "\n",
    "        tokenizer = TweetTokenizer()\n",
    "\n",
    "        all_phrases, phrase_search = read_phrases(PHRASES_FILE)\n",
    "\n",
    "        print('Initialization took {} seconds'.format((time.time() - start_time)))\n",
    "\n",
    "        # read tweets and split into sentences and tokens\n",
    "        print('Performing phrase annotation and POS tagging...')\n",
    "        tagging_start_time = time.time()\n",
    "\n",
    "        all_sents = []\n",
    "        vocab_pos = {}\n",
    "        # row = 0\n",
    "        with open(INPUT_FILE, 'r') as f:\n",
    "            with open(OUTPUT_FILE, 'w') as o:\n",
    "                for line in f:\n",
    "                    sentences = sent_tokenize(line)\n",
    "                    for raw_sent in sentences:\n",
    "                        raw_tokens = tokenizer.tokenize(raw_sent)\n",
    "\n",
    "                        phrase_sent = annotate_phrases(raw_sent, phrase_search)\n",
    "                        phrase_tokens = tokenizer.tokenize(phrase_sent)\n",
    "\n",
    "                        pos = pos_tag(raw_tokens)\n",
    "                        \n",
    "                        json.dump({\n",
    "                            'raw_sent' : raw_sent,\n",
    "                            'raw_tokens' : raw_tokens,\n",
    "                            'phrase_sent' : phrase_sent,\n",
    "                            'phrase_tokens' : phrase_tokens,\n",
    "                            'pos' : pos\n",
    "                            }, o)\n",
    "                        o.write('\\n')\n",
    "\n",
    "                        all_sents.append(phrase_tokens)\n",
    "\n",
    "                        for (word, tag) in pos:\n",
    "                            if word not in vocab_pos:\n",
    "                                vocab_pos[word.lower()] = defaultdict(int)\n",
    "                            vocab_pos[word.lower()][tag] += 1 \n",
    "\n",
    "                    # row = row + 1\n",
    "                    # if row == 2000: break\n",
    "\n",
    "        nouns = get_nouns(vocab_pos, all_phrases)\n",
    "\n",
    "        print('Tagging took {} seconds'.format((time.time() - tagging_start_time)))\n",
    "\n",
    "        # generate word embeddings\n",
    "        print('Generating word embeddings...')\n",
    "        embedding_start_time = time.time()\n",
    "\n",
    "        model = Word2Vec(sentences = all_sents)\n",
    "        model.save(os.path.join(OUTPUT_FOLDER, 'word2vec.model'))\n",
    "        print('Embedding took {} seconds'.format((time.time() - embedding_start_time)))\n",
    "\n",
    "        print('Identifying needs and priorities...')\n",
    "        detection_start_time = time.time()\n",
    "\n",
    "        get_ranked_needs(model, nouns, NEEDS_FILE, TOPN)\n",
    "        \n",
    "        print('Needs detection took {} seconds'.format((time.time() - detection_start_time)))\n",
    "\n",
    "        print('END: whole process took {} seconds'.format((time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = str(Path.cwd()) + \"/TweetData\"\n",
    "files = glob.glob(os.path.join(p,\"*.npy\"))\n",
    "\n",
    "for f in files:\n",
    "    print(os.path.basename(f).split('/')[0])\n",
    "    \n",
    "    dataset = np.load(f, allow_pickle=True).tolist()\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df = df[df[\"lang\"] == \"en\"]\n",
    "    # TokClean(df)\n",
    "    df[[\"full_text\", \"id\" ]].to_csv('outp.csv',encoding='utf-7', index=False, errors=\"replace\")\n",
    "\n",
    "    gsnd = os.path.basename(f).split('/')[-1][0:10] \n",
    "    print(gsnd)\n",
    "\n",
    "    myfunc(\"outp.csv\", \"GS\" + gsnd, \"1000\")\n",
    "\n",
    "    prio = \"WordData/\" + \"WORDS\" + gsnd \n",
    "    os.makedirs(prio)\n",
    "    priority(\"outp.csv\", prio, \"gsndData\\GS\" + gsnd)\n",
    "\n",
    "\n",
    "    # priority(\"gsndData\\GS\" + gsnd, prio, \"gsndData\\GS\" + gsnd)\n",
    "    \n",
    "    # clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13a9ced40d5452a406e42e6e8b0ff97af9777b9c935afce9e10b78419227a6d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
